{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_and_processing(path):\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in lines:\n",
    "        parts = line.split(' ', 1)\n",
    "        if len(parts) > 1:\n",
    "            # we subtact 1 to conver the dataset into a list of [0,1] and not [1,2]\n",
    "            label = int(parts[0].replace('__label__', '')) - 1\n",
    "            text = parts[1].strip()\n",
    "            labels.append(label)\n",
    "            texts.append(text)\n",
    "    \n",
    "    data = pd.DataFrame({'label': labels, 'text': texts})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a smaller sample size of 50,000 reviews for training, and 10,000 for testing. \n",
    "dir = '../Datasets/amazon sentiment analysis'\n",
    "train_dir = os.path.join(dir, 'train.txt')\n",
    "test_dir = os.path.join(dir, 'test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = loading_and_processing(train_dir)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = loading_and_processing(test_dir)\n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "\n",
    "max_len = 200\n",
    "\n",
    "# There is not need for train_test_split since we have predefined train and test files\n",
    "X_train = tokenizer.texts_to_sequences(train_data['text'])\n",
    "X_test = tokenizer.texts_to_sequences(test_data['text'])\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "y_train = train_data['label'].values\n",
    "y_test = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train size: {len(X_train)}, y_train size: {len(y_train)}\")\n",
    "print(f\"X_test size: {len(X_test)}, y_test size: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(max_len,)))\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "PREDS = (preds > 0.5).astype(int).flatten()\n",
    "\n",
    "PREDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values, counts = np.unique(PREDS, return_counts=True)\n",
    "value_counts = dict(zip(unique_values, counts))\n",
    "\n",
    "print(f'0 Counts: {value_counts[0]}.')\n",
    "print(f'1 Counts: {value_counts[1]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred, zero_division=False))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Result Analysis\n",
    "\n",
    "In analyzing the final modelâ€™s performance, I focused on its classification accuracy across sentiment classes. The model achieved an overall accuracy of 90.17% during training and 89.76% on the test set. I also examined precision, recall, and F1 score metrics, which revealed that the model performs consistently well with a macro F1 score of 0.90 for both sentiment classes.\n",
    "\n",
    "The detailed metrics indicated that the model has strong performance for both sentiment classes. Specifically, the precision for class 0 (negative sentiment) was 0.88 and for class 1 (positive sentiment) was 0.91, while the recall was 0.91 and 0.88, respectively. This shows balanced performance, with slight variation in precision and recall between the classes.\n",
    "\n",
    "Based on these insights, I can refine the model further by focusing on any minor imbalances or enhancing its ability to distinguish between sentiments more precisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
